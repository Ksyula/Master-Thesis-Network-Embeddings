\chapter{Conclusion}
\label{cha:conclusion}

The final chapter of this thesis includes the overview of the concept, discusses some critical aspects of the thesis, and concludes with potential future improvements and modifications.

\section{Overview of the thesis}

The overall goal of this thesis is to design and develop a concept and a data science pipeline for unsupervised node segmentation in financial transaction networks that find groups of users similar in the sense of structure and time.

Initially, the thesis discussed domain-specific features, which justify the motivation behind the problem statement. Some existing solutions for the same problem were presented along with their limitations. The chapter~\ref{cha:background} presented existing approaches in machine learning domain which are extensively used in the concept. The special attention was paid to the network embedding learning approaches as it is in the core of the developed pipeline. Various research works on this topic explored static networks employing random walk models. A separate group of approaches emphasized the evolution of network structure. And there was only few research available on embedding features learning with existing network attributes incorporation. These works played an important role in this thesis, especially Node2ves~\cite{node2vec} and Role2vec~\cite{role2vec}. The former proposed breakthrough sampling strategy in networks and the latter developed a heuristic on nodes' structural similarity. Sections~\ref{Cluster Analysis} and~\ref{Dimensionality Reduction} covered traditional and state-of-the-art unsupervised machine learning methods, their usage scenarios, functioning features, and limitations, which were highlighted by the authors of competing methodologies. Thereby, the chapter~\ref{cha:background} aggregates insights from more than 50 external competent sources and provides a comprehensive summary of every presented technique. Such that it brings an understanding of the choice and effect of a particular technique.

The concept and design chapter describes the proposed concept in general and at the same time provides the necessary level of details. It includes the conceptual illustration of the branched data science pipeline divided into main stages (\autoref{fig:Fig28}) along with the content of each stage. The main focus of the chapter is on the unique strategy sampling for network embeddings learning, which allows to incorporate available network attributes into Node2vec semi-random walks. In the frame of this work, the time and degree attributes were selected. Potentially, one could select other attributes too. Additionally, the chapter covers the preparatory and final procedures as the data pre-processing and results' evaluation steps respectively. 

The following implementation chapter explained how the concept was implemented including the justifications of chosen technologies, used frameworks and tools. The data pre-processing step was built taking the provided data set into consideration. Different data sets will require different adjustment of pre-processing techniques in order to produce meaningful result using the developed user segmentation pipeline. The automated feature learning framework described in section~\ref{Automated Feature Learning Framework} was implemented with the help of Node2vec 0.2.1 framework and several other common Python packages. The common machine learning techniques were implemented with the help of the recent existing frameworks listed in~\ref{Overall data science pipeline for user segmentation}. All the pipeline's stages are bundled together in a Jupyter notebook named Pipeline. The notebook contains the results evaluation and interpretation sections with illustrations, comments, and explanations. Besides the notebook, another valuable outcome of the work is the web analytics application with some artificially generated data sets for the overall approach comprehension.

The last evaluation chapter presents various insights with the help of plots and figures of intermediate and final results. The outcomes are specific to the provided data set. The quality of a clustering result is defined by the presence or absence of strong structural and/or time patterns hidden in the input network.

\section{Critical review}

There are some limitations of the current concept as well as its implementation that came out of different reasons. Some aspects are be discussed below.

\textbf{Application scenario.}
The data science pipeline has been tested on restricted real-world data set limited in the number of observations. In general, this is not the best practice to manage the initial data, while all machine learning techniques only benefit from a large data set. The decision to reduce the data was made to facilitate the visualisation of observations in the cost of accuracy.

\textbf{Data pre-processing.}
The data pre-processing step is specific to input data and user-defined. Although some intuitive heuristics on the time and degree classes split have been proposed, uncertainty still remains. Multiple runs of the pipeline with different splits on the same input data can help to develop some intuition in this regard and find a better split.

\textbf{Implementation.}
The pipeline is able to derive features from three possible combinations of time, local, and global structural components (loc + time, loc + glob, loc + time + glob). These three initial options are implemented in three different Jupyter notebooks. Probably, it is worth to come up with one file and implement this option via the input parameter of the pipeline. However, in this case, one file might become overloaded with the analytical illustrations and accompanying explanations for 18 branches results.

\textbf{Interpretation of the results.}
The obtained result strongly depends on the input data and user-defined parameters. To interpret the outcome, one should bear in mind the initial characteristics of the input data, such as time span, transaction distribution over time, degree distribution in the network, and other related basic statistics. It can help a lot during the interpretation of the pipeline result. Furthermore, some knowledge about the specific domain and data set origin are also of great help. In general, the interpretation task is very challenging and this work does not provide a generic solution. Therefore, the presented analysis is suitable for advanced network exploration in addition to the basic exploratory data analysis.

\section{Future work and research}

The developed approach is another try to learn a meaningful feature representation from dynamic networks which could potentially explain the underlying network structure. This thesis presents a novel technique for sampling from a graph based on local structures of nodes' neighbourhoods, global structural graph attribute - degree, and time of transaction represented by edges of the graph. However, there is still a large scope for improvement and modification to the developed approach.
\begin{itemize}
    \item \textbf{Consider a directed graph. }In transaction data, users play different roles sending and receiving transactions. With high probability, on average the behavioural pattern for a sender will differ from that of a receiver in terms of surrounding structures, number of connections and operating time. A directed graph distinguishes between senders and receivers. The preliminary analysis in this regard might help to interpret the pipeline result and reveal new insights of user segmentation. Furthermore, the Node2vec framework can process directed networks. However, the proposed sampling strategy has to be revised.
    \item \textbf{Consider a weighted graph. }The obvious enhancement of processing a financial network is considering the amount of transferred assets. It is an edge-based attribute and is available in the standard Node2vec framework.
    \item \textbf{More intensive evaluation. }The current evaluation employs the set of internal clustering measures for globular and non-globular clusters. $k$-means clustering produces globular clusters, and HDBSCAN produces clusters of arbitrary shapes. The overall results are compared by the reduced set of parameters which make sense for both techniques, although this set could be extended for better evaluation and comparison.
    \item \textbf{Finer hyperparameters tuning. }Although the pipeline provides semi-automated techniques on hyperparameters optimization, they can be improved and extended by the introduction of a finer scale and new optimizing techniques.
    \item \textbf{Experiments on another financial data set. }To confirm or deny the obtained conclusions, it would be beneficial to test the pipeline on another financial data set of different structure and origin.
\end{itemize}
To conclude, this thesis offers various insights about financial networks. Its users are nodes of the network and can be categorized based on their neighbourhoods in the network and graph-related attributes. The data science pipeline reveals the similarities between users according to their structural and time features within a network. This thesis explains how these similarities were obtained and how users were segmented under the minimum input information about participants. The thesis explains the concept and provides its implementation along with the web application to fully comprehend the idea.
